{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init stable diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler, StableDiffusionUpscalePipeline\n",
    "import torch\n",
    "\n",
    "def initdiffusionpipeline(model_id):\n",
    "    # Use the DPMSolverMultistepScheduler (DPM-Solver++) scheduler here instead\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "    return pipe\n",
    "\n",
    "diffusionpipeline=initdiffusionpipeline(\"stabilityai/stable-diffusion-2-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initupscalepipeline(model_id):\n",
    "    pipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "    pipeline.enable_attention_slicing() \n",
    "    pipeline = pipeline.to(\"cuda\")\n",
    "    return pipeline\n",
    "\n",
    "upscalepipeline=initupscalepipeline(\"stabilityai/stable-diffusion-x4-upscaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateimage(prompt,seed=42):\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "    image = diffusionpipeline(prompt,generator=generator).images[0]\n",
    "    fname = f\"images/{prompt.replace(' ','-')}.png\"\n",
    "    image.save(fname)\n",
    "    display(image)\n",
    "    return image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv()) # take environment variables from .env.\n",
    "assert os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\") is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initllm(model_id):\n",
    "    # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "    return HuggingFaceHub(repo_id=model_id, model_kwargs={\"temperature\": 0.1, \"max_length\": 64})\n",
    "\n",
    "# search LLMs for stable diffusion prompting:\n",
    "# https://huggingface.co/models?search=stable%20diffusion%20prompt\n",
    "# llmchain = initllmchain(\"DrishtiSharma/StableDiffusion-Prompt-Generator-GPT-Neo-125M\")\n",
    "llm = initllm(\"Ar4ikov/gpt2-650k-stable-diffusion-prompt-generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateprompt(basicprompt):\n",
    "    betterprompt = llm(basicprompt)\n",
    "    print(betterprompt)\n",
    "    return betterprompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myprompt=\"a photo of an astronaut riding a horse on mars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img=generateimage(myprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betterprompt = myprompt+generateprompt(myprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betterimg=generateimage(betterprompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
